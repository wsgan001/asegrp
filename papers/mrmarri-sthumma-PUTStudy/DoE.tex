\begin{table*}[t]%
\centering
\begin{minipage}{\textwidth}
\centering
\begin{tabular}{|l|r|r|r|r|r|r|r|r|r|}
%----------------- HEADER ------------------------
\hline
Subject 		& Downloads 	& \multicolumn{5}{|c|}{Code Under Test} 															&	\multicolumn{3}{|c|}{Existing Test Code} 	\\ \cline{3-10}
						& 						&	\#Classes	&	\#Methods	& \#KLOC	& Avg. Complexity & Max. Complexity	&	\#Classes	&	\#CUTs	 	&	\#KLOC						\\ \hline\hline
%----------------- END HEADER ------------------------ 
NUnit				&		$193,563$\footnote{Number of reads as shown in sourceforge for the year 2009.}
													&	9					&	87  			&		1.4		&				1.48 			& 14 							&			9			&			49		&		0.9							\\ \hline
DSA					&		$2241$		&	27  			&	259 			&		2.4		&				2.09 			& 16							&			20		&			337		&		2.5							\\ \hline		
QuickGraph	&		$7969$ 		&	56				&	463				&		6.2		&				1.79			& 16							&			9			&			21		&		1.2							\\ \hline
\end{tabular}
%\footnotetext[1]{Anonymous read count in the year 2009}
\end{minipage} \vspace*{-3ex}
\caption{Details of the subject applications.} \vspace*{-3ex}
\label{tab:subjectmetrics}
\end{table*} 

\section{Empirical Study}
\label{sec:study}

We conducted an empirical study using three real-world applications to show the benefits of generalizing CUTs to PUTs in specific and also to show the applicability of our systematic procedure in general. In our empirical study, we show the benefits of PUTs over existing CUTs in terms of three metrics: branch coverage, the number of detected defects, and the number of tests (their LOC) being reduced by test generalization. In particular, we address the following three research questions in our empirical study:

\begin{itemize}
	\item \textbf{RQ1: Branch Coverage.} How much higher percentage of \emph{branch coverage} is achieved by PUTs compared to existing CUTs? Since PUTs are a generalized form of CUTs, this research question helps to address whether PUTs can achieve additional branch code coverage compared to CUTs.
	\item \textbf{RQ2: Defect Detection.} How many new \emph{defects} (that are not detected by CUTs) are detected by PUTs and vice-versa? This research question helps to address whether PUTs have higher fault-detection capabilities compared to CUTs.
	\item \textbf{RQ3: Test-code maintenance.} How many tests are reduced by generalizing CUTs to PUTs? This research question helps to address whether PUTs require less effort in maintaining test code compared to CUTs.
\end{itemize}

To address these research questions, we execute the existing CUTs (of each application) and the unit tests generated by Pex with the help of the transformed PUTs, and measure the preceding three metrics. Initially, we execute the existing CUTs and meticulously record the achieved branch coverage and also the failing tests as defects. In addition, we measure the number of CUTs and lines of code (LOC) of CUTs for all applications. We then generalize these CUTs to PUTs based on our systematic procedure and use Pex to generate unit tests. We then execute the new tests generated from PUTs and record achieved branch coverage, the number of failing tests, the number of PUTs, and the number of LOC of PUTs. Furthermore, we manually analyze all failing tests to ensure that the failing tests are not due to invalid PUTs, such as missing assumptions on the parameters. We thus confirm that the finally recorded failing tests are due to defects in the code under test.

For measuring the branch coverage of the code under test, we use a coverage measurement tool, called NCover\footnote{http://www.ncover.com/}. For measuring the code metrics such as LOC and cyclomatic complexity, we use the Count Lines of Code\footnote{http://cloc.sourceforge.net/} (CLOC) tool.

\subsection{Subject Applications}

We use three open source applications in our study: NUnit~\cite{nunit}, DSA~\cite{dsa}, and Quickgraph~\cite{quickgraph}. The criteria for choosing these applications are (1) popular usage of these applications in industry as shown by their download records in their hosting web sites, (2) the existing CUTs, (3) the number of lines of the code under test of these applications, and (4) the code under test itself including non-trivial logic to validate the feasibility of our systematic procedure. Table~\ref{tab:subjectmetrics} shows the characteristics of the three subject applications. Column ``Subject'' gives the name of the subject application. Column ``Downloads'' gives the number of downloads of the application as listed in its hosting web site. Column ``Code Under Test'' gives details of the code under test (of the application) in terms of the number of classes (``\#Classes''), number of methods (``\#Methods''), number of lines of code (``\#LOC''), and the average complexity of the code under test. Column ``Existing Test Code'' provides details on the existing test code similar to the details of the code under test. Subcolumns ``\#Classes'', ``\#Methods'', and ``\#LOC'' give the corresponding metrics of the test code. We next provide details of all the subject applications.

\subsubsection{NUnit}
\label{sec:nunit}
We use an open source application, called NUnit~\cite{nunit}, as one of the subject applications for our study. NUnit, a counterpart of JUnit for Java~\cite{JUnit}, is a widely used open source unit-testing framework for all .NET languages. NUnit is written in C\# and uses attribute-based programming model~\cite{TDD} through a variety of attributes such as \CodeIn{[TestFixture]} and \CodeIn{[Test]}. The rationale behind choosing NUnit for test generalization is its popularity  (increase in the usage from $450$ reads in the year 2002 to $193,563$ in the year $2009$) as listed by its hosting website SourceForge\footnote{http://sourceforge.net/}. Furthermore, the large number of manually written unit tests available with the application makes NUnit a suitable subject for test generalization. For the purpose of the study, we used nine classes from the \CodeIn{util} package (\CodeIn{nunit.util.dll}), which is one of the core components of the framework. We chose the \CodeIn{util} package with two reasons: (1) it is one of the basic modules developed for the framework and (2) it is an independent module and is not dependent on the other modules of the NUnit framework. We chose the nine classes as the code under test since the logic in these classes is non-trivial; the average complexity is $1.48$ (maximum of 14) as shown in Table~\ref{tab:subjectmetrics}. For our study, we used NUnit release version $2.4.8$. 

\subsubsection{DSA}

Data Structures and Algorithms (DSA)~\cite{dsa} is a library that contains implementation of data structures and algorithms, a few of which are not available in the .NET 3.5 framework. There are two major packages in the library's source code, \CodeIn{Dsa.DataStructures} and \CodeIn{Dsa.Algorithms}. The DSA library includes $27$ classes and $259$ methods. The existing test suite available with the DSA library includes $20$ test classes and $337$ CUTs. The rationale behind choosing DSA is the size of its test suite available with the release version $0.6$. The existing test suite includes CUTs that test all the classes of the DSA source code. Furthermore, another reason for choosing DSA for our study is the influence of DSA in the software industry as shown by the number of downloads, a total of $2,236$ downloads as shown by its hosting site, Codeplex\footnote{http://codeplex.com/}. 

\subsubsection{QuickGraph}

QuickGraph~\cite{quickgraph} is a C\# graph library that provides various directed/undirected graph data structures. QuickGraph also provides algorithms such as depth-first search, breadth-first search, and A* search. The source code of QuickGraph (version $1.0$) used in our study includes $56$ classes and interfaces with $6.2$ KLOC. The existing test suite available with the source code release of the library includes $9$ test classes and $21$ CUTs. The existing test suite includes CUTs that primarily test two search algorithms, one sorting algorithm, and other graph concepts. We generalized only these existing CUTs of this subject application. Since QuickGraph includes graph search algorithms, the code under test includes non-trivial logic and is therefore a suitable subject for test generalization. QuickGraph is also popularly used as reflected by $7,969$ downloads (shown by CodePlex).

\input{coverage}
\input{defects}
\input{maintenance}