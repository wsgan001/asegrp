\begin{table*}[t]%
\centering
\begin{minipage}{\textwidth}
\centering
\begin{tabular}{|l|r|r|r|r|r|r|r|r|}
%----------------- HEADER ------------------------
\hline
Subject 		& Downloads 	& \multicolumn{4}{|c|}{Code Under Test} 					&	\multicolumn{3}{|c|}{Existing Test Code} \\ \cline{3-9}
						& 						&	\#Classes	&	\#Methods	& \#LOC	& Avg. Complexity	&	\#Classes	&	\#Methods &	\#LOC		\\ \hline\hline
%----------------- END HEADER ------------------------ 
NUnit				&		$193,563$\footnote{Anonymous read count in the year 2009}
													&						&						&				&									&						&						&					\\ \hline
DSA					&		$2241$		&						&						&				&									&						&						&					\\ \hline		
QuickGraph	&		$7969$ 		&						&						&				&									&						&						&					\\ \hline
\end{tabular}
%\footnotetext[1]{Anonymous read count in the year 2009}
\end{minipage}
\caption{Details of the subject applications}
\label{tab:subjectmetrics}
\end{table*}

\section{Empirical Study}

We conducted an empirical study using three real world applications to study the benefits of Parameterized Unit Tests (PUT) over Conventional Unit Tests (CUT) in specific and to show the applicability of the proposed systematic procedure in general. In our empirical study, we show the benefits of PUTs over existing CUTs in terms of three metrics: code coverage, defects, and test code maintenance. In particular, we address the following three research questions through our empirical study:

\begin{itemize}
	\item \textbf{RQ1: Coverage.} How much higher percentage of \emph{code coverage} is achieved by PUTs compared to existing CUTs? Since PUTs are a generalized form of CUTs, this research question helps to address whether PUTs can achieve additional coverage compared to CUTs.
	\item \textbf{RQ2: Defects.} How many new \emph{defects} (that are not detected by CUTs) are detected by PUTs and vice-versa? This research question helps to address whether PUTs have more fault-detection capabilities compared to CUTs.
	\item \textbf{RQ3: Test code maintenance.} How many number of tests are reduced by generalizing CUTs to PUTs? This research question helps to address whether PUTs require lesser efforts for maintaining test code compared to CUTs.
\end{itemize}

To address these research questions, we execute the existings CUTs of each application and execute the unit tests generated by Pex with the help of the transformed PUTs and measure the three metrics. Initially, we execute the existing CUTs and meticulously record the code coverage achieved and note the failing test cases as defects. In addition to that, we measure the number of CUTs and lines of code of each of the CUTs. We then generalize these CUTs to PUTs based on our systematic procedure. Consequently, we apply Pex using the generalized PUTs and record the coverage achieved, the number of failing test cases, the number of PUTs, and the number of lines of these PUTs. Furthermore, we manually analyze to ensure that the failing test cases did not fail due to invalid PUTs, such as missing assumptions on the parameters. We thus confirm that the test cases failed due to a defect in the code under test. \\

\noindent\textbf{Metrics.}
For our study, we measure two major metrics, (1) Branch coverage (2) Code metrics such as LOC and code complexity. For measuring the branch coverage of the code under test, we use a coverage measurement tool called NCover\footnote{http://www.ncover.com/}. For measuring the code metrics such as LOC and code complexity, we use the Count Lines of Code\footnote{http://cloc.sourceforge.net/} (CLOC) tool.

\subsection{Subject Applications}

We chose three open source applications to carry out our study, NUnit~\cite{nunit}, DSA~\cite{dsa}, and Quickgraph~\cite{quickgraph}. The criteria for choosing these projects are (1) large usage of the application in terms of the download records shown by the hosting site, (2) Existing conventional unit test suite size, (3) the size of the application sources, and (4) the code under test should contain non-trivial logic to validate the feasibility of test generalization. Table~\ref{tab:subjectmetrics} shows the characteristics of the three subject applications. Column ``Subject'' gives the name of the subject applications and Column ``Downloads'' gives the number of downloads of the applications as listed on the respective hosting websites. Column ``Code Under Test'' gives details of the code under test in the corresponding application in terms of the number of classes (``\#Classes''), number of methods (``\#Methods''), number lines of code (``\#LOC''), and the average complexity of the code under test. Column ``Existing Test Code'' provides details on the existing test code corresponding to the code under test. Subcolumns ``\#Classes'', ``\#Methods'', and ``\#LOC'' give the corresponding test code's metrics. In the following sections, we provide details on each of the subject applications.

\subsubsection{NUnit}
\label{sec:nunit}
We use an open source application, called NUnit~\cite{nunit}, as one of the subject applications for our study. NUnit, a counterpart of JUnit for Java~\cite{JUnit}, is a widely used open source unit-testing framework for all .NET languages. NUnit is written in C\# and uses attribute-based programming model~\cite{TDD} through a variety of attributes such as \CodeIn{[TestFixture]} and \CodeIn{[Test]}. The rationale behind choosing NUnit for test generalization is its popularity in terms of the number of downloads ($450$ anonymous reads in the year 2002 and $193,563$ in the year $2009$) of the application as listed by its hosting website SourceForge\footnote{http://sourceforge.net/}. Furthermore, the large number of manually written unit tests available with the project makes NUnit a good subject for test generalization. For the purpose of the study, we used $9$ classes from the \CodeIn{util} package (\CodeIn{nunit.util.dll}), which is one of the core components of the framework. We chose the \CodeIn{util} package in the study for two reasons: (1) it is one of the basic modules to be developed for the framework (2) it is an independent module and is not dependent on the other modules of the NUnit framework. We chose the $9$ classes as the code under test since the logic in these classes is non-trivial; the average complexity is $0$ as shown in Table~\ref{tab:subjectmetrics}. For our study, we used NUnit release version $2.4.8$. 

\subsubsection{DSA}

Data Structures and Algorithms (DSA)~\cite{dsa} is a library that contains implementation of data structures and algorithms, a few of which are not available in the .NET 3.5 framework. There are two major packages in the library's source code, \CodeIn{Dsa.DataStructures} and \CodeIn{Dsa.Algorithms}. The library containt $0$ classes and $0$ methods. The existing test suite available with the source code release of the library contains a total of $0$ test classes and $0$ test cases. The rationale behind choosing DSA is the size of its test suite available with the release version $0.6$. The existing test suite containted test cases that test most of the source code. Furthermore, another reason for choosing DSA for our study is the influence in terms of the large number of downloads, a total of $2236$ downloads as shown by its hosting site, Codeplex\footnote{http://codeplex.com/}. 

\subsubsection{QuickGraph}

QuickGraph~\cite{quickgraph} is a C\# graph library that provides various directed/undirected graph data structures. QuickGraph also provides algorithms such as depth-first search, breadth-first search, and A* search. QuickGraph includes $165$ classes and interfaces with $5$ KLOC. The existing test suite available with the source code release of the library contains a total of $0$ test classes and $0$ test cases. The existing test suite containted test cases that majorly test two search algorithms, one sorting algorithm, and graph concepts. We generalized only these existing test cases again for this subject, i.e., the code under test contains $0$ classes and $0$ methods. Since QuickGraph includes graph search algorithms, the code under test includes non-trivial logic and is therefore a good subject for our test generalization. QuickGraph is also popularly used reflected by the number of downloads, $7969$ as shown by CodePlex.

\input{coverage}
\input{defects}
\input{maintenance}