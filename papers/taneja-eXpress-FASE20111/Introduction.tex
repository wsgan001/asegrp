\vspace{-5ex}
\section{Introduction}
\label{sec:intro}
\vspace{-1ex}
Regression test generation aims at generating a test suite that can detect behavioral differences between the original and the new versions of a program. A behavioral difference between two versions of a program can be reflected by the difference between the observable outputs produced by the execution of the same test (referred to as a difference-exposing test) on the two versions. Developers can inspect these behavioral differences to determine whether they are intended or unintended (i.e., regression faults).

Regression test generation can be automated by using Dynamic Symbolic
Execution (DSE)~\cite{dart,cute}, a state-of-the-art test generation
technique, to generate a test suite achieving high
structural coverage. DSE explores paths in a program to
achieve high structural coverage, and exploration of all
these paths can often be expensive. However, if our aim is
to detect behavioral differences between two versions of a
program, we do not need to explore all these paths in the program
since not all these paths are relevant for detecting behavioral
differences.

To formally investigate irrelevant paths for exposing behavioral differences, we adopt the 
Propagation, Infection, and Execution (PIE) model~\cite{voas} of error propagation. According to the PIE model, a fault can be detected by a test if a faulty statement is executed (E), the execution of the faulty statement infects the state (I), and the infected state (i.e., error) propagates to an observable output (P). A change in the new version of a program can be treated as a fault and then the PIE model is applicable for effect propagation of the change. Our key insight is that many paths in a program often cannot help in satisfying any of the conditions P, I, or E of the PIE model. 

In this paper, we present an approach{\footnote{\scriptsize{An earlier version~\cite{taneja09:guided} of this work is described in a four-page paper that appears in the NIER track of ICSE 2009. This
work significantly extends the previous work in the following major ways.
%First, in this paper, we develop techniques for efficiently finding irrelevant branches 
%that cannot execute any change. 
First, we develop techniques for exploiting the existing test suite for efficiently generating regression tests.
Second, we develop techniques for efficiently achieving state infection after a change is executed.
Third, we automate our approach by developing a tool.
Fourth, we conduct extensive experiments to evaluate our approach.}} \CodeIn{eXpress} and its implementation that uses DSE to detect behavioral differences based on the notion of the PIE model.
Our approach first determines all the branches (in the program under test) that cannot help in achieving any of the conditions E, I, and P of the PIE model in terms of the changes in the program. To make test generation efficient, we develop a new search strategy for DSE to avoid exploring these irrelevant branches (including which can lead to an irrelevant path\footnote{\scriptsize{An irrelevant path is a path that cannot help in achieving P, I, and E of the PIE model.}}). In particular, our approach guides DSE to avoid from flipping branching nodes\footnote{\scriptsize{A branching node in the execution tree of a program is an instance of a conditional statement in the source code. A branching node consists of two sides (or more than two sides for a \CodeIn{switch} statement): the true branch and the false branch. Flipping a branching node is flipping the execution of the program from the true (or false) branch to the false (or true) branch. Flipping a branching node for a switch statement is flipping the execution of the current branch to another unexplored branch.}}, which on flipping execute some irrelevant branch. 
In addition, to effectively achieve condition I of the PIE model, our approach instruments a program under test with additional branches, 
such that if any of these branches are executed, program state is infected. 
Furthermore, our approach can exploit the existing test suite (if available) for the original version by seeding the tests in the test suite to the program exploration. Our technique of seeding the exploration with the existing test suite efficiently augments an existing test suite so that various changed parts of the program (that are not covered by the existing test suite) are covered by the augmented test suite. 

\Comment{In addition, our approach prioritizes the flipping of branching nodes\footnote{A branching node in the execution tree of a program is an instance of a conditional statement in the source code. A branching node consists of two sides (or more than two sides for a \CodeIn{switch} statement): the true branch and the false branch. Flipping a branching node is flipping the execution of the program from the true (or false) branch of the branching node to the false (or true) branch.  Flipping a branching node representing a switch statement is flipping the execution of current branch to some unexplored branch.} in such a manner that behavioral differences are more likely to be detected earlier in path exploration.
}

This paper makes the following major contributions:
%\\ \textbf{Problem Definition.} We formally define the problem of regression test generation.
\\ \textbf{Path Exploration for Regression Test Generation.} We propose an approach called \CodeIn{eXpress} that uses DSE for efficient generation of regression tests. 
\CodeIn{eXpress} prunes paths that cannot help in achieving conditions E, I, and P.
In addition, \CodeIn{eXpress} instruments the program such that condition I can be achieved effectively
after condition E is achieved. 
%To the best
%of our knowledge, ours is the first approach that guides path exploration on the new version specifically for regression test generation.
\\ \textbf{Incremental Exploration.} We develop a technique for exploiting an existing test suite, so that path exploration focuses on covering the changes rather than starting from scratch. 
%To the best of our knowledge, ours is the first technique that leverages an existing test suite for automated regression test generation.
\\ \textbf{Implementation.} We have implemented our \CodeIn{eXpress} approach in a tool as an extension for Pex~\cite{Pex},  an automated structural testing tool for .NET developed at Microsoft Research. Pex has been previously used internally at Microsoft to test core components of the .NET architecture and has found serious
faults~\cite{Pex}. The current Pex has been downloaded tens of thousands of times in industry. 
\\ \textbf{Evaluation.} We have conducted experiments on 67 versions (in total) of four programs. 
Experimental results show that our approach requires about 36\% fewer amount of time (on average) to detect behavioral differences than exploration without using our approach. In addition, our approach detects 6 behavioral difference that could not be detected by exploration without using our approach (within a time bound).
%(2) 44\% fewer runs to cause program-state differences after its execution than exploration without using our approach, 
%and **\% fewer abount of time and **\% fewer runs to find behavioral differences. 
Furthermore, our approach requires 67\% less amount of time to find behavioral differences by exploiting an existing test suite than exploration without using the test suite. 
\Comment{
Experimental results show that our approach requires about 53\% fewer runs (i.e., explored paths) on average to cause the execution of a changed region\footnote{\scriptsize{A changed region is a minimal set of statements that contains all the changed statements in a method.}}, 44\% fewer to cause program-state differences after its execution than exploration without using our approach, and **\% fewer runs to find behavioral differences. 
In addition, our approach requires 71\% fewer runs to cover all the changed regions by exploiting an existing test suite than exploration without using the test suite. 
}
%\end{itemize}
\Comment{
The rest of the paper is organized as follows. Section~\ref{sec:example} 
presents an example to illustrate our approach. Section~\ref{sec:approach} presents 
our approach and the major components involved in the approach. Section~\ref{sec:evaluation}
presents the evaluation results. Section~\ref{sec:validity} discusses the threats to validity of the evaluation results. Section~\ref{sec:related} discusses related
work. Section~\ref{sec:discussion} discusses research issues and future work,
and Section~\ref{sec:conclusion} concludes.
}