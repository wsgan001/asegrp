\begin{abstract}
\Comment{
During its life cycle, a software system undergoes various changes. Regression testing is the activity of testing these changes made to the software system. In particular, regression testing aims to expose behavioral differences between the original and changed versions of the software system. Regression testing is an important activity to ensure software quality. However, regression testing is one of the most expensive activity during software maintenance. Various approaches that have been proposed in literature focus on reducing the cost of regression testing through test suite prioritization, selection or minimization. However, as these approaches do not specifically aim to test the changed parts of the software systems, many behavioral differences cannot be detected. Other approaches that aim to specifically test the changed parts of a software system are expensive.

In this paper, we present an approach that takes two versions of a software system as input and generates a test suite, which on execution exposes behavioral differences between two versions of the software system. Our approach, inserts additional branches in the source code such that if these branches can be covered behavioral differences can be found between the two versions. Our approach then uses a symbolic-execution-based approach to generate test cases that covers these branches. To make the test generation process efficient, our approach guides the symbolic execution engine to avoid exploring branches that cannot help directly or indirectly in exposing behavioral differences between the two versions of software system.
**Discuss Experimental Results**
}
\Comment{
Regression test generation aims at generating test inputs that can detect behavioral differences between the original and the modified version of a software system. Although these differences can be detected by using a test suite achieving high structural coverage on both the versions but white-box test generation techniques are often not scalable to large software systems because of a huge number of paths in large software systems. However, if our aim is to detect behavioral differences between two versions of a software system, we do not need to explore many paths in the software system as these paths are not useful in detecting behavioral differences. In this paper, we propose an approach that uses Dynamic Symbolic Execution (DSE) for effective
generation of regression unit tests. In particular, our approach develops a new search strategy for DSE to avoid exploring irrelevant
paths that cannot help in detecting behavioral differences. In addition, our approach prioritizes the flipping
of branching nodes such that behavioral differences can be detected effectively.
**Discuss Preliminary Results**
}

\Comment{
Regression test generation aims at generating a test suite that can detect behavioral differences between the original and the new versions of a software system. Dynamic Symbolic Execution (DSE) can be used to generate test suite achieving high coverage that can detect behavioral differences between the two versions of a software system. However, DSE has to explore many paths in the software system to achieve a high structural coverage of the software system. Exploration of all these paths can be expensive. However, if our aim is to detect behavioral differences between two versions of a software system, we do not need to explore many paths in the software system as these paths are not useful in detecting behavioral differences. In this paper, we propose an approach that uses Dynamic Symbolic Execution (DSE) for effective
generation of regression tests. In particular, our approach develops a new search strategy for DSE to avoid exploring irrelevant
paths that cannot help in detecting behavioral differences. In addition, our approach prioritizes the flipping
of branching nodes such that behavioral differences are more likely to be detected earlier than other search strategies. Preliminary results show that our approach requires around 17\% less runs to detect behavioral differences than the default search strategy in Pex, a DSE based structural testing tool.
}
Regression test generation aims at generating a test suite that can detect behavioral differences between the original and the new versions of a program. Regression test generation can be automated by using Dynamic Symbolic Execution (DSE), a state-of-the-art test generation technique. DSE explores all feasible paths in the program but exploration of all these paths can often be expensive. However, if our aim is to detect behavioral differences between two versions of a program, we do not need to explore all 
these paths in the program, since not all these paths are relevant for detecting behavioral differences. In this paper, we propose an approach on guided path exploration that avoids exploring irrelevant paths in terms of detecting behavioral differences. 
\textbf{
In addition, our approach guides the path exploration towards infecting the program state by instrumenting the program with additional branches such that execution of these branches implies state infection.
}
Hence, behavioral differences are more likely to be detected earlier in path exploration.
 Furthermore, our approach leverages the existing test suite (if available) for the original version to efficiently execute the changed regions of the program and infect program states. 
Experimental results on 67 versions (in total) of four programs show that our approach requires about 36\% fewer amount of time (on average) to detect behavioral differences than exploration without using our approach. In addition, our approach detects 6 behavioral difference that could not be detected by exploration without using our approach (within a time bound).
%(2) 44\% fewer runs to cause program-state differences after its execution than exploration without using our approach, 
%and **\% fewer abount of time and **\% fewer runs to find behavioral differences. 
Furthermore, our approach requires 67\% less amount of time to find behavioral differences by exploiting an existing test suite than exploration without using the test suite. 

\end{abstract}