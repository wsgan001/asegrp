
\section{Related Work}
\label{sec:related}
Previous approaches~\cite{evans:DiffTest07,taneja08:diffgen,jin10:automated} generate regression unit tests achieving high structural coverage on both versions of the class under test. However, these approaches explore all the irrelevant paths, which cannot help in achieving any of the conditions I or E in the PIE model~\cite{voas}. In contrast, we have developed a new search strategy for DSE to avoid exploring these irrelevant paths. 

Santelices et al.~\cite{santelices08sep} use data and control dependence information along with state information gathered through symbolic execution, and provide guidelines for testers to augment an existing regression test suite. Unlike our approach, their approach does not automatically generate tests but provides guidelines for testers to augment an existing test suite. 

Some existing search strategies~\cite{burnim,fitnex} guide DSE to efficiently achieve high structural coverage in a program under test. However,
these techniques do not specifically target covering a changed region. In contrast, our approach guides DSE to avoid exploring paths that cannot help in satisfying any of the conditions P, I, or E of the PIE model.

Differential symbolic execution~\cite{DSE} determines behavioral differences between two versions of a method (or a program) by comparing their symbolic summaries~\cite{CSE}. Summaries can be computed only for methods amenable to symbolic execution. However, summaries cannot be computed for methods whose behavior is defined in external libraries not amenable to symbolic execution. Our approach still works in practice when these external library methods are present since our approach does not require summaries. In addition, both approaches can be combined using demand-driven-computed summaries~\cite{demandDriven}, which we plan to investigate in future work.

%Our previous Orstra approach~\cite{xie06:ecoop} automatically augments an automatically generated test suite with extra assertions for guarding against regression faults. Orstra first runs the given test suite and collects the return values and receiver-object states after the execution of the methods under test. Based on the collected information, Orstra synthesizes and inserts new assertions in the test suite for asserting against the collected method-return values and receiver object states. However, this approach observes the behavior of the original version to insert assertions in the test suite generated for only the original version. Therefore, the  test suite might not include test inputs for which the behavior of a new version differs from the original version.


%Li~\cite{jjli:prioritization} prioritizes source code portions for testing based on dominator analysis. In particular, her approach finds a minimal set of blocks in the program source code, which, if executed, would ensure the execution of all of the blocks in the program. Howritz~\cite{howritz:prioritization} prioritizes  portions of source code for testing based on control and flow dependencies. These two approaches focus on testing in general. In contrast, our approach focuses specifically on regression testing.
}

%Ren et al. develop a change impact analysis tool called Chianti~\cite{chianti}. Chianti uses a test suite to produce an execution trace for two versions of a program, and then
%categorizes and decomposes the changes between two versions of a program into different atomic types. Chianti uses only an existing test suite and
%does not generate new tests for regression testing. In contrast, our approach focuses on regression test generation.
%
%Some existing capture and replay techniques~\cite{elbaum,orso:selective,SaffAPE2005} capture
%the inputs and outputs of the unit under test during system-test execution. 
%These techniques then replay the captured inputs for the
%unit as less expensive unit tests, and can also check the outputs of
%the unit against the captured outputs. However, the existing system tests do not necessarily exercise the changed behavior of the program under test. In contrast, our approach generates new tests for regression testing.


%Joshi et al.~\cite{joshi:pretext} use the path constraints of the paths followed by the tests in an existing test suite to generate inputs that violate the assertions in the test suite. The generated test inputs follow the same paths already covered by the existing test suite and do not explore any new paths. In contrast, our approach exploits the existing test suite to explore new paths. Majumdar and Sen~\cite{majumdar:hybrid} propose the concept of hybrid concolic testing. Hybrid concolic testing seeds the program with random inputs so that the program exploration does not get stuck at a particular program location. In contrast, our approach exploits the existing test suite to seed the program exploration. Since the existing test suite is expected to achieve a higher structural coverage, the existing test suite is expected to discover more hard-to-discover branching nodes in comparison with random inputs. 
Godefroid et al.~\cite{godefroid:fuzz} propose a DSE based approach for fuzz testing of large applications. Their approach uses a single seed for program exploration. In contrast, our approach  seeds multiple tests to program exploration. Seeding multiple tests can help program exploration in covering the changes more efficiently as discussed in Section~\ref{sec:incremental}. 

%Law and Rothermel~\cite{Law-icse03} propose an impact analysis technique, called PathImpact. 
%PathImpact uses method execution traces to find out impacted 
%methods when a method is modified. Our technique of building an inter-procedural graph 
%by pruning certain method call chains that are found to be reachable to a changed region
%is similar to the technique. 
%However, our technique works on a static interprocedural graph due to which our technique is safe in contrast to 
%PathImpact. In addition, our technique further reduces the size of Inter-procedural graph by not adding the 
%already visited methods from the inter-procedural graph.

%Rothermel and Harrold~\cite{rothermel97} introduce the notion of dangerous 
%edges and use these dangerous edges for regression test selection. 
%Our irrelevant branches for execution and infection (set $B_{E+I}$) of 
%a changed region is the inverse of these dangerous edges.
%However, our approach also finds irrelevant branches that cannot help in propagating 
%a state infection to observable output.

Xu and Rothermel~\cite{xu-directed} propose a directed test generation technique that uses 
the existing test suite to cover parts of the program that are not covered by 
the existing test suite. In particular, the approach first collects 
the set of branches $B$ that are not covered by the existing test suite. To cover a branch 
$b_i =<v_i,v_j> \in B$, the approach selects all the tests $T$ that cover the vertex $v_i$. 
For each test $t_i \in T$, the approach collects the path constraints $p_i$ of path followed by $t_i$ until $v_i$, 
negates the predicate at $v_i$ from $p_i$ to get path condition $p_i'$.
The approach then generates a test that covers the branch 
$b_i$ by solving the path conditions $p_i$ . 
However if all the path conditions of paths followed by the tests $T$ are not solvable, the approach cannot 
generate a test to cover the branch $b_i$, which can furthermore compromise the coverage of additional branches.
In contrast, our incremental exploration technique can still generate a test to cover such branches.
In addition, the approach focuses only on satisfying condition E of the PIE model, while our approach
helps in satisfying E, I, and P of the PIE model to find behavioral differences.

%
%Jin et al.~\cite{jin10:automated} propose an approach, called BERT, for behavioral regression testing.
%BERT finds all the classes in a program that are modified and generates tests for these classes 
%that capture the behavior of these classes.
%BERT then executes these tests on the original version to find behavioral differences.
%In contrast to our approach, BERT does not focuses on efficiently generating 
%tests to find behavioral differences. Hence, our approach is complementary to BERT.



