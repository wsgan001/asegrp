\section{Introduction}
\label{sec:intro}

Software maintenance is an expensive phase of software development 
life cycle. Software maintenance involves maintaining programs that 
continuously evolve during their life time. One important aspect 
of software maintenance is to make sure that the changes introduced 
in the new version of software do not introduce any unwanted
side effects in the existing functionality. Regression testing is a testing 
methodology that aims at exposing such unwanted side effects introduced
in the new version of software. Majority of software
maintenance costs is spent for regression testing~\cite{}.

A regression test is a form of unit test that is created on one
version of software (often a stable version) and is executed 
on further versions of software. In particular, a unit test includes three 
major components: test scenario, test data, and test
assertions. Figure~\ref{fig:sampleunittest} shows an example unit test.
Test scenario refers to the method-call sequence shown in Statements
1, 2, and 3. Test data refers to the concrete values (such as 7 and 3
in Statements 2 and 3, respectively) passed as arguments to the method calls. 
Test assertions refer to assertions (Statement 4) that verify whether the actual
behavior is the same as the expected behavior.

Although unit tests are widely used, writing unit tests manually is tedious and expensive. TODO: Put
some statistics over here to show how expensive to write unit tests.
Recent advances in the state-of-the-art of software testing introduced Parameterized Unit Tests (PUT),
that address the issue of test data. TODO: Explain here advantages of PUTs and how conventional unit
tests can be generated back from PUTs using dynamic symbolic execution.

TODO: insert code for PUT

Although PUTs address the issue of test data, these PUTs do not address the issue of test scenarios,
which are also one of the major components of a unit test. Writing test scenarios manually is also
an expensive task. However, it is quite challenging to generate these test scenarios automatically
due to a large search space of possible sequences and relevant sequences (define what are relevant
sequences?) are quite small. TODO: Put more sentences here of why it is that challenging

In this paper, we propose an approach that addresses the issue of test scenarios by automatically 
generating those scenarios from dynamic traces recorded during program execution. In our approach,
we chose dynamic traces compared to static traces, as dynamic traces are more precise compared
to static traces. Furthermore, dynamic traces also include concrete values along with method-call
sequences. In our approach, we later use these concrete values as seed values for dynamic
symbolic execution. Section~\ref{} presents more details on how we use seed values for increasing
the effectiveness of dynamic symbolic execution in achieving more coverage.

TODO: To describe about happy paths covered by recorded traces and how our approach generates
unit tests that cover all other paths in the blocks covered by test scenarios.

TODO: Put some related work here to show how we are different from other approaches and 
to show that our approach is novel. Also, mention that Pex is used as a dynamic symbolic 
execution tool.

\begin{figure}[t]
\begin{CodeOut}
\begin{alltt}
00:void AddTest() \{
01:\hspace*{0.2in}HashSet set = new HashSet();
02:\hspace*{0.2in}set.Add(7);
03:\hspace*{0.2in}set.Add(3);
04:\hspace*{0.2in}Assert.IsTrue(set.Count == 2);
05:\}
\end{alltt}
\end{CodeOut}
\Caption{\label{fig:sampleunittest} An example unit test.}
\end{figure}

Our approach includes three major phases. In the first phase, referred to as the \emph{capture} phase,
we record dynamic traces during program execution. These dynamic traces include two major aspects: realistic scenarios
of API calling sequences and concrete values passed to those API method calls. 
We transform each dynamic trace into a PUT, where all primitive types are promoted as
arguments. We generate unit tests from the concrete values available in the dynamic traces.
As these traces are collected during program execution, we identify that most of the traces are duplicates.
Consequently, we have many duplicate PUTs and unit tests. We consider a PUT, say $P_1$, as a duplicate
of another PUT, say $P_2$, if both $P_1$ and $P_2$ have the same sequence of method calls. Similar,
we consider a unit test, say $U_1$, as a duplicate of another unit test, say $U_2$, if both $U_1$
and $U_2$ exercise the same execution path. In our second phase, referred to as the \emph{minimize}
phase, we address this issue of duplicate PUTs and seed unit tests by using a combination of static
and dynamic analysis. In the third phase, referred to as \emph{explore} phase, we generate new unit
tests from PUTs using dynamic symbolic execution. We use seed tests to assist dynamic symbolic
execution. These seed tests help increase the effectiveness of dynamic symbolic execution in 
achieving higher code coverages (as shown in our evaluation). In our approach, we identify
even after minimization of PUTs and seed unit tests, the remaining number of PUTs are
still large, leading to scalability issues with dynamic symbolic execution. To address this issue,
we develop a distributed setup, where we can perform dynamic symbolic execution using multiple
machines.



This paper makes the following major contributions:

\begin{itemize}

\item An approach to record dynamic traces during program execution and generate PUTs and seed
unit tests from recorded dynamic traces.
\item An approach to minimize duplicate PUTs and seed unit tests by eliminating duplicate
PUTs and seed unit tests, respectively, using a combination of static and dynamic analyses.
\item A distributed approach to address the scalability issues with dynamic symbolic execution.
\item Three large-scale evaluations to show the effectiveness of our approach. 
In our approach, we recorded $\approx$1.5 GB (including 433,809) of dynamic traces related
to ten .NET base class libraries during program executions. 
We generated 68,575 PUTs and 128,185 unit tests after the minimize phase. From these PUTs,
we generated 501,799 unit tests through dynamic symbolic execution that achieved a
high code coverage of the ten .NET base class libraries. TODO: Insert more figures
and evaluation results.
\end{itemize}





