\section{Introduction}
\label{sec:intro}

Software maintenance is an important phase of the software development life cycle. Software maintenance involves maintaining programs that evolve during their life time. One important aspect of software maintenance is to make sure that the changes made in the new version of software do not introduce any unwanted side effects in the existing functionality. Regression testing is a testing methodology that aims at exposing such unwanted side effects, referred to as regression faults, introduced in the new version of software. Rosenblum and Weyuker~\cite{rosenblum96:regression} describe that the majority of software maintenance costs is spent on regression testing.

The basis of regression testing is to have high-covering unit tests on a stable version of software. It is essential to have high-covering unit tests because many types of defects such as functional defects are difficult to be detected without executing the relevant portions in the code under test. These unit tests created on one version of software are executed on the further versions of software to expose regression faults. Although regression testing is our ultimate goal, in this paper, we address
the challenge of generating high-covering unit tests on a given version of software.

In general, a unit test includes three major components: test scenario, test data, and test assertions. Figure~\ref{fig:sampleunittest} shows an example unit test. In a unit test, test scenario refers to the method-call sequence shown in Statements 1, 2, and 3. Test data refers to the concrete values (such as 7 and 3 in Statements 2 and 3, respectively) passed as arguments to the method calls. Test assertions refer to assertions (Statement 4) that verify whether the actual behavior is the same as the expected behavior.

\begin{figure}[t]
\begin{CodeOut}
00:void AddTest() \{\\
01:\hspace*{0.2in}HashSet set = new HashSet();\\
02:\hspace*{0.2in}set.Add(7);\\
03:\hspace*{0.2in}set.Add(3);\\
04:\hspace*{0.2in}Assert.IsTrue(set.Count == 2);\\
05:\}
\end{CodeOut}\vspace*{-4ex}
\Caption{\label{fig:sampleunittest} An example unit test.}
\begin{CodeOut}
00:void AddSpec(int x, int y) \{\\
01:\hspace*{0.2in}HashSet set = new HashSet();\\
02:\hspace*{0.2in}set.Add(x);\\
03:\hspace*{0.2in}set.Add(y);\\
04:\hspace*{0.2in}Assert.AreEqual(x == y, set.Count == 1);\\
05:\hspace*{0.2in}Assert.AreEqual(x != y, set.Count == 2);\\
06:\}
\end{CodeOut}\vspace*{-4ex}
\Caption{\label{fig:sampleput} An example PUT.}\vspace*{-2ex}
\end{figure}

Recent advancements in software testing introduced Parameterized Unit Tests (PUT)~\cite{tillmann05:parameterized}, which generalize conventional unit tests by accepting parameters. Figure~\ref{fig:sampleput} shows a PUT for the unit test shown in Figure~\ref{fig:sampleunittest}, where concrete values in Statements 2 and 3 are replaced by the parameters \CodeIn{x} and \CodeIn{y}. An approach, called dynamic symbolic execution~\cite{Clarke:symbolic, godefroid:dart, king:symex, koushik:cute}, can be used to automatically generate a small set of conventional unit tests that achieve a high coverage of the code under test defined by PUT. Section~\ref{sec:background} provides more details on how dynamic symbolic execution generates conventional unit tests from PUTs. In our approach, we use Pex~\cite{tillman:pexwhite}, a state-of-the-art 
dynamic symbolic execution engine. However, our approach is not specific to Pex, and can be used with any other test input generation engine.

A major advantage of PUTs compared to conventional unit tests is that test data is automatically generated based on the code under test. However, writing meaningful PUTs
can still be challenging since PUTs require realistic test scenarios (method-call sequences) to exercise the code under test. Automatic generation of test scenarios is quite challenging due to a large search space of possible scenarios and only a few scenarios are meaningful in practice. In literature, there exist three major categories of approaches that generate test scenarios in the form of method-call sequences: bounded-exhaustive~\cite{khurshid:symbolic, xie:rostra}, evolutionary~\cite{inkumsah08:improving, tonella:etoc}, and random~\cite{csallner:jcrasher, JTEST, pacheco:feedback}. However, these approaches are either not scalable or not effective in practice due to their random nature~\cite{thummalapenta09:mseqgen}. 

Our approach addresses the issue of test scenarios by automatically generating test scenarios from dynamic traces recorded during program execution. We use dynamic traces compared to static traces, since dynamic traces are more precise than static traces. These dynamic traces include two aspects: realistic scenarios of method-call sequences and concrete values passed as arguments to those method calls. Since recorded dynamic traces include both test scenarios and test data (concrete values passed as arguments to method calls in test scenarios), regression tests can directly be generated from the recorded dynamic traces. However, such regression tests exercise only \emph{happy paths} such
as paths that do not include error-handling code in the code under test. To address this issue, we first transform recorded dynamic traces into PUTs. We use concrete values in dynamic traces to generate conventional unit tests that are used as seed tests to increase the efficiency of dynamic symbolic execution while exploring PUTs~\cite{patrice08:whitebox}.

Since the dynamic traces are recorded during program execution, we identify that many of recorded traces are duplicates. The reason for duplicates is that the same method-call sequences can get invoked multiple times. Consequently, we have many duplicate PUTs and seed tests. Exploration of such duplicate PUTs is redundant and can also lead to scalability issues. Therefore, we first filter out duplicate PUTs and seed tests by using static and dynamic analyses, respectively. We next explore the remaining PUTs to generate regression tests that can achieve a high coverage of the code under test. In our evaluations (and also in practice), we identify that even after minimization of PUTs and seed tests, the number of remaining PUTs and seed tests can still be large, and it would take a long time (days or months) to explore those PUTs with dynamic symbolic execution on a single machine. To address this issue, we develop a distributed setup that allows parallel exploration of PUTs. To infer test assertions, we execute the generated regression tests on a stable version of software and capture the return values of method calls in the regression. We generate test assertions from these captured
return values. These test assertions help detect regression faults by checking whether the new version of software also returns the same values.

To the best of our knowledge, ours is the first scalable approach that automatically generates regression tests without requiring any manual efforts. In our evaluations, we show that our approach records $\approx$ 1.5GB (size of C\# source code) of dynamic traces and generates $\approx$ 500,000 regression tests on two core libraries of .NET 2.0 framework. These statistics show that our approach is scalable and can be used in practice to deal with large real-world applications.

In summary, this paper makes the following major contributions:

\begin{itemize}
\item Given a set of recorded dynamic traces, a technique to transform them into PUTs and seed unit tests.
\item A technique to filter out duplicate PUTs and seed unit tests by using static and dynamic analyses, respectively.
\item A distributed setup for the parallel exploration of PUTs to generate conventional unit tests.
\item Three large-scale evaluations to show the effectiveness of our approach. In our approach, we recorded $\approx$1.50 GB C\# source code (including 433,809 traces) of dynamic traces related to two core libraries .NET framework. From these PUTs, our approach generated 501,799 regression tests that achieved a high code coverage (24.3\% higher than the coverage achieved by seed unit tests) of two core .NET framework libraries.
\end{itemize}

The rest of the paper is structured as follows:
Section~\ref{sec:background} presents background on a DSE-based approach.
Section~\ref{sec:approach} describes key aspects of our approach.
Section~\ref{sec:eval} presents our evaluation results.
Section~\ref{sec:threats} discusses threats to validity.
Section~\ref{sec:future} discusses limitations of our approach and future work.
Section~\ref{sec:related} presents related work.
Finally, Section~\ref{sec:concl} concludes. 





