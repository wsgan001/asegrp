\section{Introduction}
\label{sec:intro}

Software maintenance is an important phase of software development 
life cycle. Software maintenance involves maintaining programs that 
evolve during their life time. One important aspect 
of software maintenance is to make sure that the changes made 
in the new version of software do not introduce any unwanted
side effects in the existing functionality. Regression testing is a testing 
methodology that aims at exposing such unwanted side effects introduced
in the new version of software. Rosenblum and Weyuker~\cite{rosenblum96:regression} describe
that the majority of software maintenance costs is spent on regression testing.

A regression test is a unit test (also referred to as a conventional unit test) that is created on one version of software 
(often a stable version) and is executed on further versions of software
to expose unwanted side effects. In general, a unit test includes three major components: test scenario, test data, and test
assertions. Figure~\ref{fig:sampleunittest} shows an example unit test.
In a unit test, test scenario refers to the method-call sequence shown in Statements
1, 2, and 3. Test data refers to the concrete values (such as 7 and 3
in Statements 2 and 3, respectively) passed as arguments to the method calls. 
Test assertions refer to assertions (Statement 4) that verify whether the actual
behavior is the same as the expected behavior.

\begin{figure}[t]
\begin{CodeOut}
00:void AddTest() \{\\
01:\hspace*{0.2in}HashSet set = new HashSet();\\
02:\hspace*{0.2in}set.Add(7);\\
03:\hspace*{0.2in}set.Add(3);\\
04:\hspace*{0.2in}Assert.IsTrue(set.Count == 2);\\
05:\}
\end{CodeOut}\vspace*{-4ex}
\Caption{\label{fig:sampleunittest} An example unit test.}
\begin{CodeOut}
00:void AddSpec(int x, int y) \{\\
01:\hspace*{0.2in}HashSet set = new HashSet();\\
02:\hspace*{0.2in}set.Add(x);\\
03:\hspace*{0.2in}set.Add(y);\\
04:\hspace*{0.2in}Assert.AreEqual(x == y, set.Count == 1);\\
05:\hspace*{0.2in}Assert.AreEqual(x != y, set.Count == 2);\\
06:\}
\end{CodeOut}\vspace*{-4ex}
\Caption{\label{fig:sampleput} An example PUT.}\vspace*{-2ex}
\end{figure}

Recent advancements in software testing introduced Parameterized
Unit Tests (PUT)~\cite{tillmann05:parameterized}, which generalize conventional unit tests 
by accepting parameters. Figure~\ref{fig:sampleput} shows a PUT for the unit
test shown in Figure~\ref{fig:sampleunittest}, where concrete values in Statements 2 and 3
are replaced by the parameters \CodeIn{x} and \CodeIn{y}. An approach, called dynamic symbolic
execution~\cite{Clarke:symbolic, godefroid:dart, king:symex, koushik:cute},
can be used to automatically generate a minimal set of conventional unit tests
that achieve a high coverage of the code under test defined by PUT.
Section~\ref{sec:background} provides more details 
on how dynamic symbolic execution generates conventional unit tests from PUTs. 
In our approach, we use Pex~\cite{tillman:pexwhite} as an example state-of-the-art 
dynamic symbolic execution approach. A major advantage of PUTs compared to 
conventional unit tests is that test data is automatically
generated based on the code under test. However, writing good PUTs
can still be challenging since PUTs require test scenarios (method-call sequences) to exercise the code under test. 
Automatic generation of test scenarios is quite challenging due to a 
large search space of possible scenarios and valid scenarios are quite small.
In literature, there exist three major categories 
of approaches that generate test scenarios in the form of method-call sequences:
bounded-exhaustive~\cite{khurshid:symbolic, xie:rostra}, evolutionary~\cite{inkumsah08:improving, tonella:etoc}, 
and random~\cite{csallner:jcrasher, JTEST, pacheco:feedback}. In our previous
work~\cite{thummalapenta09:mseqgen}, we show that these approaches are either not scalable or not
effective in practice due to their random nature. 

Our approach addresses the issue of test scenarios by automatically generating test scenarios
from dynamic traces recorded during program execution. 
We use dynamic traces compared to static traces, since dynamic traces are more 
precise than static traces. These dynamic traces include two aspects: realistic scenarios
of method-call sequences and concrete values passed as arguments to those method calls.
Since recorded dynamic traces include both test scenarios and
test data (concrete values passed as arguments to method calls in test scenarios),
regression tests can directly be generated from the recorded dynamic traces.
However, such regression tests exercise only happy paths such
as paths that do not include error-handling code in the code under test.
To address this issue, we first transform recorded dynamic traces
into PUTs. We use concrete values in dynamic traces to generate
conventional unit tests that are used as seed tests to increase
the efficiency of dynamic symbolic execution while exploring PUTs~\cite{patrice08:whitebox}.

Since the dynamic traces are recorded during program execution, 
we identify that many of recorded traces are duplicates. The 
reason for duplicates is that the same method-call sequences
can get invoked multiple times. Consequently, we have many duplicate PUTs and seed tests. 
Exploration of such duplicate PUTs is redundant and can also
lead to scalability issues. Therefore, we first filter
out duplicate PUTs and seed tests by using static and 
dynamic analyses, respectively. We next explore the remaining
PUTs to generate regression tests that can achieve a high coverage
of the code under test. In our evaluations (and also in practice), 
we identify that even after minimization of PUTs and seed tests, 
the remaining number of PUTs can still be many and can take long
time in exploring those PUTs. To address this issue, we develop a 
distributed setup that allows parallel exploration of PUTs.
To infer test assertions, we execute the generated regression
tests on a stable version of software and capture the return values
of observer methods. These observer methods are pure methods
that do not change the state of their receiver or arguments. For example,
\CodeIn{Hashset.Count} shown in Statement 4 of Figure~\ref{fig:sampleunittest} is
an observer method.

To the best of our knowledge, ours is the first scalable
approach that automatically generates regression tests without 
requiring any manual efforts. In our evaluations, we show that our approach handles 
$\approx$ 1.5GB of dynamic traces and generates $\approx$ 500,000 regression tests
on ten .NET 2.0 framework base class libraries. These numbers show that our approach
is scalable and can be used in practice to deal with large real-world applications.

In summary, this paper makes the following major contributions:

\begin{itemize}
\item A technique to record dynamic traces during program execution and generate PUTs and seed
unit tests from recorded dynamic traces.
\item A technique to filter out duplicate PUTs and seed unit tests by 
using static and dynamic analyses, respectively.
\item A distributed setup for the parallel exploration of PUTs to generate conventional
unit tests.
\item Three large-scale evaluations to show the effectiveness of our approach. 
In our approach, we recorded $\approx$1.5 GB (including 433,809) of dynamic traces related
to ten .NET framework base class libraries. From these PUTs,
our approach generated 501,799 regression tests that achieved a
high code coverage of the ten .NET framework base class libraries. 
\end{itemize}

The rest of the paper is structured as follows:
Section~\ref{sec:background} presents background on a DSE-based approach.
Section~\ref{sec:example} explains our approach with an example.
Section~\ref{sec:approach} describes key aspects of our approach.
Section~\ref{sec:eval} presents our evaluation results.
Section~\ref{sec:threats} discusses threats to validity.
Section~\ref{sec:future} discusses limitations of our approach and future work.
Section~\ref{sec:related} presents related work.
Finally, Section~\ref{sec:concl} concludes. 





